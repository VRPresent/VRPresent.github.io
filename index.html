<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">


<title>V R Present</title>

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">V R Present</h1>
      <h2 class="project-tagline"></h2>
      
      
    </section>

    <section class="main-content">
      <h2 id="VRPresent"><strong>V R Present</strong></h2>
<p>V R Present is a frugal Teleassistance, immersive Telepresence application</p>

<h1 id="abstract">Introduction</h1>
<p>  Globalization has led to the distribution of corporate operations across multiple geographies. As a result, employees are commonly required to participate in interactions with remote workspaces. Even though video-conferencing is increasingly being used to make this possible, interacting with static displays feels too impersonal and lacks a physical dimension. Telepresence and Tele-robotics aim to bridge this gap, and in this paper, we describe a frugal setup that provides a more natural and immersive experience for the participating entities, with the help of virtual reality on one end, and a robotic avatar representing the human on the other side of the communication. In addition, the same framework can also be applied to the domains of Tele-Surveillance, wherein a human can control a robot to monitor a remote area from a safe vantage point, and Tele-assistance, wherein experts can remotely guide on-site employees. The frugality stems from the use of basic Android phones on either end of the communication, and from the fact that the software for controlling the robot has been written generically so as to support a large variety of robots, as per need. Such a system would help reduce the time, cost and effort spent by organizations and employees on travel logistics, as well as provide an option for off-site operation, while providing a better and more control-rich simulation of physical presence than standard video-conferencing.
</p>

<h1 id="video">Video</h1>
<div class="embed-container">
  <iframe src="https://www.youtube.com/embed/BGRY14znFxY" width="700" height="480" frameborder="0" allowfullscreen="">
  </iframe>
</div>



<!-- Lines 40-71 have been commented
 -->

<h1 id="idea">Architechture</h1>

<figure>
<p><img src="refine.jpeg" alt="Figure 1 " /></p>	
<figcaption>Figure 1 : The left side represents the pilot side where the user is situated while the right side represents the remote/robot side.</figcaption>	
</figure>

<figure>
<p><img src="ROS.png" alt="Figure 1 " /></p>	
<figcaption>Figure 2 : V R Present Robot Side Architechture </figcaption>	
</figure>



<p>
Our system is entirely based on android and uses WebRTC framework for android to android communication.
The android devices on both sides are connected by a two-way WebRTC A/V channel, for real-time audio-video communication, as well as a WebRTC data channel. This data channel is used by the pilot to send control commands to the robot using voice, gamepad / joystick movements, or even head motion. Head tracking and stereoscopic AR / VR rendering is done using the Google Cardboard SDK.

The commands sent by the pilot are received by the V R Present Android application on the robot side. 
Feedbacks regarding the button pressed, the mode switches are given by providing audio feedback using android text to speech library.

As shown in the architecture diagram (Figure 2) , any ROS-enabled (Robot Operating System enabled) mobile robot can connect to this application. The robot has it's own ROS environment with a rosmaster up and running. This ROS environment consists of two major ROS nodes - a custom built V R Present Package for tele-operation, and the robot's ROS driver package (Robot Driver Package). The V R Present Package has a UDP server running which receives data from the Android application, extracts velocity information from it, and publishes that on the /cmd\textunderscore vel topic. This topic is, in turn, subscribed to by the Robot Driver Package for translational and rotational movement of the robot. The Robot Driver Package also publishes odometry data on the /odom topic, which is subscribed to by the V R Present Package for feedback in head tracking mode.
</p>

<!-- <h1 id="candf">Conclusions and Future Work</h1>

<p><img src="fd.jpg" alt="" /></p>

<p>We presented V R Present, a frugal immersive framework for teleassistence,telepresence, and remote surveillance. Telepresence mode has various applications in monitoring and surveillance, as well as in meetings and presentations, while Teleassist mode has uses in the field of industrial training and repair tasks. Our work combines the best of both in the same application. Application is flexible, immersive , easy to use, and cost effective compared to the counterparts discussed.
In future, we plan to extend our work by incorporating in device deep learning models for detecting user engagement in presentations, by using three dimensional audio for a more immersive experience and by incorporating sharing of virtual objects between the two sides in TeleAssist mode. 
</p>
 -->

<!-- <h1 id="results">Results</h1>

<ul>
  <li>
    <p><strong>Using CycleGAN</strong><br />
<img src="https://egogestvid.github.io/imgs/fig_inputoutput-1.jpg" alt="no-alignment" /></p>
  </li>
  <li>
    <p><strong>Synthesised using our approach.</strong><br /></p>
  </li>
</ul>
<p align="center">
  <img height="50%" width="50%" src="/imgs/domain_shift-1.jpg" />
</p>

<p><br />Clearly, images are more clearer and the segmentation masks give us control over the fingertip location, handâ€™s appearance, shape, size and so on.</p>
 -->

<!-- # Video
<p align="center">
<a href="https://youtu.be/M5ADsP4l9Zo" target="_blank"><img height="50%" width="50%" src="/imgs/video.png" border="50" /></a>
</p>
 -->


      <footer class="site-footer">
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </section>

    
  </body>
</html>
