<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">


<title>V R Present</title>

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">V R Present</h1>
      <h2 class="project-tagline"></h2>
      
      
    </section>

    <section class="main-content">
      <h2 id="VRPresent"><strong>V R Present</strong></h2>
<p>A frugal framework for telepresence applications.</p>

<h1 id="abstract">Introduction</h1>
<p>  Globalization has led to the distribution of corporate operations across multiple geographies. As a result, employees are commonly required to participate in interactions with remote workspaces. Even though video-conferencing is increasingly being used to make this possible, interacting with static displays feels too impersonal and lacks a physical dimension. Telepresence and Tele-robotics aim to bridge this gap, and in this paper, we describe a frugal setup that provides a more natural and immersive experience for the participating entities, with the help of virtual reality on one end, and a robotic avatar representing the human on the other side of the communication. In addition, the same framework can also be adapted to the domains of Tele-Surveillance, wherein a human can control a robot to monitor a remote area from a safe vantage point, and Tele-assistance, wherein experts can remotely guide on-site employees. The frugality stems from the use of basic Android phones on either end of the communication, and from the fact that the software for controlling the robot has been written generically so as to support a large variety of robots, as per need. Such a system would help reduce the time, cost and effort spent by organizations and employees on travel logistics, as well as provide an option for off-site operation, while providing a better and more control-rich simulation of physical presence than standard video-conferencing.
</p>





<!-- Lines 40-71 have been commented
 -->

<h1 id="idea">Architecture</h1>
<p>
Figures depicting overall system and architecture is described below.
</p>

<figure>
<p><img src="refine.jpeg" alt="Figure 1 " /></p>	
<figcaption>Figure 1 : The left side represents the pilot side where the user is situated while the right side represents the remote/robot side.</figcaption>	
</figure>

<figure>
<p><img src="ROS.png" alt="Figure 1 " /></p>	
<figcaption>Figure 2 : V R Present Robot Side Architechture </figcaption>	
</figure>


<h1 id="video">Video</h1>
<div class="embed-container">
  <iframe src="https://www.youtube.com/embed/gDKC2uHzrMA" width="700" height="480" frameborder="0" allowfullscreen="">
  </iframe>
</div>


<!-- <h1 id="candf">Conclusions and Future Work</h1>

<p><img src="fd.jpg" alt="" /></p>

<p>We presented V R Present, a frugal immersive framework for teleassistence,telepresence, and remote surveillance. Telepresence mode has various applications in monitoring and surveillance, as well as in meetings and presentations, while Teleassist mode has uses in the field of industrial training and repair tasks. Our work combines the best of both in the same application. Application is flexible, immersive , easy to use, and cost effective compared to the counterparts discussed.
In future, we plan to extend our work by incorporating in device deep learning models for detecting user engagement in presentations, by using three dimensional audio for a more immersive experience and by incorporating sharing of virtual objects between the two sides in TeleAssist mode. 
</p>
 -->

<!-- <h1 id="results">Results</h1>

<ul>
  <li>
    <p><strong>Using CycleGAN</strong><br />
<img src="https://egogestvid.github.io/imgs/fig_inputoutput-1.jpg" alt="no-alignment" /></p>
  </li>
  <li>
    <p><strong>Synthesised using our approach.</strong><br /></p>
  </li>
</ul>
<p align="center">
  <img height="50%" width="50%" src="/imgs/domain_shift-1.jpg" />
</p>

<p><br />Clearly, images are more clearer and the segmentation masks give us control over the fingertip location, handâ€™s appearance, shape, size and so on.</p>
 -->

<!-- # Video
<p align="center">
<a href="https://youtu.be/M5ADsP4l9Zo" target="_blank"><img height="50%" width="50%" src="/imgs/video.png" border="50" /></a>
</p>
 -->


      <footer class="site-footer">
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </section>

    
  </body>
</html>
